# -*- coding: utf-8 -*-
"""Code mémoire 01-05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E76VDmcSFACV5nWqCCSae6a_255ArQFF
"""

# STOCK MARKET PERFORMANCE PREDICTION PROJECT
# Includes data collection, preprocessing, visualization, entropy analysis, and modeling

# STEP 1: IMPORT LIBRARIES
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, SimpleRNN, Conv1D, MaxPooling1D, Flatten, Dropout
import statsmodels.api as sm
from statsmodels.tsa.api import VAR
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings("ignore")

# STEP 2: DOWNLOAD DATA
# US Stocks + Indexes
symbols = ['AAPL', 'MSFT', 'GOOGL', 'META', 'AMZN', 'TSLA', 'NVDA', 'JPM', 'V', 'UNH',
           'XOM', 'JNJ', 'WMT', 'PG', 'MA', 'HD', 'BAC', 'KO', 'PFE', 'DIS',
           '^GSPC', '^DJI', '^IXIC']  # S&P 500, Dow Jones, Nasdaq
start_date = '2015-01-01'
end_date = datetime.today().strftime('%Y-%m-%d')
data = yf.download(symbols, start=start_date, end=end_date)['Adj Close']
data = data.dropna()
data.to_csv('raw_stock_data.csv')

# Multimodal Stock Market Forecasting Notebook
# Predicting US Stock Indexes using News Sentiment, Macroeconomic Variables, and Deep Learning Models

# --- 1. INSTALL & IMPORT LIBRARIES ---
!pip install yfinance --quiet
!pip install fredapi --quiet
!pip install pyinform --quiet

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from datetime import datetime
from fredapi import Fred
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

from keras.models import Sequential
from keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, SimpleRNN
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

import pyinform
from pyinform.transferentropy import transfer_entropy

# --- 2. FETCH STOCK INDEX DATA FROM YAHOO FINANCE ---
# S&P 500, Nasdaq, Dow Jones
symbols = ['^GSPC', '^IXIC', '^DJI']
start_date = '2015-01-01'
end_date = '2024-12-31'

raw_data = yf.download(symbols, start=start_date, end=end_date, progress=False)
data = raw_data['Close'].dropna()
data.columns = ['SP500', 'NASDAQ', 'DOWJONES']

# --- 3. FETCH MACROECONOMIC VARIABLES FROM FRED ---
fred = Fred(api_key="YOUR_FRED_API_KEY")  # Replace with your FRED API Key
macro_vars = {
    'FEDFUNDS': 'Interest Rate',
    'UNRATE': 'Unemployment Rate',
    'CPIAUCSL': 'CPI'
}

macro_data = pd.DataFrame()
for code, label in macro_vars.items():
    series = fred.get_series(code, observation_start=start_date, observation_end=end_date)
    macro_data[label] = series
macro_data.index = pd.to_datetime(macro_data.index)
macro_data = macro_data.resample('D').ffill()

# Installation des bibliothèques nécessaires
!pip install pandas_datareader
!pip install yfinance --upgrade --no-cache-dir

# Import des bibliothèques nécessaires
import pandas as pd
import numpy as np
import yfinance as yf
import pandas_datareader.data as web
import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.api import VAR
from sklearn.model_selection import train_test_split

# Définir la période de récupération des données (2010 à 2024)
start = datetime.datetime(2010, 1, 1)
end = datetime.datetime(2024, 12, 31)

# Récupérer les données économiques de FRED (exemple du taux de chômage)
unemployment_data = web.DataReader('UNRATE', 'fred', start, end)

# Affichage des premières lignes des données
print(unemployment_data.head())

# Récupérer les données de plusieurs actions et indices (S&P, Nasdaq, Dow Jones)
stocks = ['AAPL', 'GOOGL', 'AMZN', 'MSFT', 'TSLA', 'FB', 'SPY', 'DJIA', 'IXIC']

# Télécharger les données financières des actions via Yahoo Finance
data = yf.download(stocks, start=start, end=end)

# Afficher les premières lignes des données financières
print(data.head())

# Sélectionner la colonne 'Adj Close' pour chaque action
adj_close_data = data['Adj Close']

# Visualiser les données de prix ajusté des actions
adj_close_data.plot(figsize=(14, 7))
plt.title('Prix Ajusté des Actions')
plt.xlabel('Date')
plt.ylabel('Prix')
plt.show()

# Normalisation des données avec MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(adj_close_data)



# Création de données d'entraînement et de test
train_size = int(len(scaled_data) * 0.8)
train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]

# Préparer les données pour l'entraînement LSTM
def create_lstm_data(data, time_step=60):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:i + time_step])
        y.append(data[i + time_step, 0])
    return np.array(X), np.array(y)

X_train, y_train = create_lstm_data(train_data)
X_test, y_test = create_lstm_data(test_data)

# Reshaper les données pour LSTM (samples, time_step, features)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Définir le modèle LSTM
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

lstm_model = build_lstm_model((X_train.shape[1], 1))

# Entraîner le modèle LSTM
lstm_model.fit(X_train, y_train, epochs=50, batch_size=32)

# Faire des prédictions
y_pred_lstm = lstm_model.predict(X_test)

# Calculer les erreurs de prédiction pour LSTM
mae_lstm = mean_absolute_error(y_test, y_pred_lstm)
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)
print(f'LSTM MAE: {mae_lstm}, MSE: {mse_lstm}, RMSE: {rmse_lstm}')

# Préparer les données pour LSTM
def create_lstm_data(data, time_step=60):
    X, y = [], []
    for i in range(time_step, len(data)):
        X.append(data[i - time_step:i, 0])  # Utiliser les 60 dernières observations
        y.append(data[i, 0])  # La prédiction sera la valeur de l'observation suivante
    return np.array(X), np.array(y)

# Assurer que la forme des données est correcte
X, y = create_lstm_data(scaled_data)

# Vérification de la forme des données
print(f"Forme de X: {X.shape}")
print(f"Forme de y: {y.shape}")

# Séparation en ensembles d'entraînement et de test
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Reshaper les données pour LSTM (samples, time_step, features)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Vérification des dimensions après le reshape
print(f"Forme de X_train après reshape: {X_train.shape}")
print(f"Forme de X_test après reshape: {X_test.shape}")

# Définir le modèle LSTM
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

lstm_model = build_lstm_model((X_train.shape[1], 1))

# Entraîner le modèle LSTM
lstm_model.fit(X_train, y_train, epochs=50, batch_size=32)

# Faire des prédictions
y_pred_lstm = lstm_model.predict(X_test)

# Calculer les erreurs de prédiction pour LSTM
mae_lstm = mean_absolute_error(y_test, y_pred_lstm)
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)
print(f'LSTM MAE: {mae_lstm}, MSE: {mse_lstm}, RMSE: {rmse_lstm}')

# Vérifier si les données contiennent des NaN
print(f"Vérification des NaN dans les données d'entraînement et de test:")
print(f"X_train contient-il des NaN ? {np.isnan(X_train).any()}")
print(f"X_test contient-il des NaN ? {np.isnan(X_test).any()}")
print(f"y_train contient-il des NaN ? {np.isnan(y_train).any()}")
print(f"y_test contient-il des NaN ? {np.isnan(y_test).any()}")

# Si des NaN sont présents, les supprimer ou les interpoler
X_train = np.nan_to_num(X_train)  # Remplacer les NaN par 0
X_test = np.nan_to_num(X_test)    # Remplacer les NaN par 0
y_train = np.nan_to_num(y_train)  # Remplacer les NaN par 0
y_test = np.nan_to_num(y_test)    # Remplacer les NaN par 0

# Après l'entraînement du modèle, vérifier si les prédictions contiennent des NaN
y_pred_lstm = model_lstm.predict(X_test)
print(f"Les prédictions contiennent-elles des NaN ? {np.isnan(y_pred_lstm).any()}")
y_pred_lstm = np.nan_to_num(y_pred_lstm)  # Remplacer les NaN par 0 si présents

# Calculer les erreurs de prédiction pour LSTM
mae_lstm = mean_absolute_error(y_test, y_pred_lstm)
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)

print(f"MAE: {mae_lstm}")
print(f"MSE: {mse_lstm}")
print(f"RMSE: {rmse_lstm}")

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Définir la structure du modèle LSTM
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Vérifier s'il y a des NaN dans les données
print(f"Vérification des NaN dans les données d'entraînement et de test:")
print(f"X_train contient-il des NaN ? {np.isnan(X_train).any()}")
print(f"X_test contient-il des NaN ? {np.isnan(X_test).any()}")
print(f"y_train contient-il des NaN ? {np.isnan(y_train).any()}")
print(f"y_test contient-il des NaN ? {np.isnan(y_test).any()}")

# Si des NaN sont présents, les supprimer ou les interpoler
X_train = np.nan_to_num(X_train)  # Remplacer les NaN par 0
X_test = np.nan_to_num(X_test)    # Remplacer les NaN par 0
y_train = np.nan_to_num(y_train)  # Remplacer les NaN par 0
y_test = np.nan_to_num(y_test)    # Remplacer les NaN par 0

# Création et entraînement du modèle LSTM
model_lstm = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
model_lstm.fit(X_train, y_train, epochs=50, batch_size=32)

# Vérification des NaN dans les prédictions
y_pred_lstm = model_lstm.predict(X_test)
print(f"Les prédictions contiennent-elles des NaN ? {np.isnan(y_pred_lstm).any()}")
y_pred_lstm = np.nan_to_num(y_pred_lstm)  # Remplacer les NaN par 0 si présents

# Calculer les erreurs de prédiction pour LSTM
mae_lstm = mean_absolute_error(y_test, y_pred_lstm)
mse_lstm = mean_squared_error(y_test, y_pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)

print(f"MAE: {mae_lstm}")
print(f"MSE: {mse_lstm}")
print(f"RMSE: {rmse_lstm}")

# Visualisation des résultats
plt.figure(figsize=(14, 7))
plt.plot(y_test, color='blue', label='Valeurs réelles')
plt.plot(y_pred_lstm, color='red', label='Prédictions LSTM')
plt.title('Prédictions LSTM vs Valeurs réelles')
plt.legend()
plt.show()