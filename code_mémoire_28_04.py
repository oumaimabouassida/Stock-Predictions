# -*- coding: utf-8 -*-
"""Code mémoire 28-04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15WhYNWeua93hvEIF3S_KDllEJ1XrvuGi
"""

# --- INSTALL PACKAGES IF NEEDED ---
!pip install yfinance pandas matplotlib seaborn statsmodels scikit-learn

# --- IMPORTS ---
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# --- STOCKS TO DOWNLOAD ---
stocks = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'META', 'TSLA', 'JPM', 'V', 'JNJ', 'WMT']

# --- MACROECONOMIC VARIABLES FROM FRED ---
fred_variables = {
    'GDP': 'GDP',                   # Gross Domestic Product
    'CPI': 'CPIAUCSL',               # Consumer Price Index
    'UNRATE': 'UNRATE',              # Unemployment Rate
    'FEDFUNDS': 'FEDFUNDS',          # Federal Funds Rate
}

# --- DOWNLOAD STOCK PRICES ---
data_stocks = yf.download(stocks, start='2015-01-01', end='2023-12-31')['Adj Close']
data_stocks = data_stocks.dropna()

# --- DOWNLOAD STOCK PRICES ---
data_stocks = yf.download(stocks, start='2015-01-01', end='2023-12-31', auto_adjust=True)['Close']
data_stocks = data_stocks.dropna()

# --- DOWNLOAD MACROECONOMIC DATA ---
from pandas_datareader import data as pdr
import datetime

start = datetime.datetime(2015, 1, 1)
end = datetime.datetime(2023, 12, 31)

data_macro = pd.DataFrame()
for name, code in fred_variables.items():
    data_macro[name] = pdr.DataReader(code, 'fred', start, end)

# --- FORWARD FILL MISSING MACRO DATA ---
data_macro = data_macro.fillna(method='ffill')

# --- RESAMPLE MACROECONOMIC DATA TO DAILY ---
data_macro = data_macro.resample('D').ffill()

# --- JOIN STOCKS + MACRO DATA ---
data = data_stocks.join(data_macro, how='inner')

print("Final data shape:", data.shape)
data.head()

# --- VISUALIZE STOCK PRICES ---
plt.figure(figsize=(16,8))
for stock in stocks:
    plt.plot(data.index, data[stock], label=stock)
plt.legend()
plt.title('Stock Prices Over Time')
plt.xlabel('Date')
plt.ylabel('Adjusted Close Price')
plt.show()

# --- VISUALIZE MACROECONOMIC VARIABLES ---
fig, axs = plt.subplots(len(fred_variables), 1, figsize=(14,10))
for i, var in enumerate(fred_variables.keys()):
    axs[i].plot(data.index, data[var])
    axs[i].set_title(var)
plt.tight_layout()
plt.show()

# --- CHECK STATIONARITY (ADF Test) ---
def adf_test(series, name):
    result = adfuller(series.dropna())
    print(f'ADF Test for {name}:')
    print(f'  Test Statistic = {result[0]:.4f}')
    print(f'  p-value = {result[1]:.4f}')
    print('  Stationary' if result[1] < 0.05 else '  Non-Stationary')
    print('')

print("\n--- ADF Tests for Stocks ---")
for stock in stocks:
    adf_test(data[stock], stock)

print("\n--- ADF Tests for Macroeconomic Variables ---")
for macro in fred_variables.keys():
    adf_test(data[macro], macro)

# --- DATA PREPROCESSING ---

# 1. LOG RETURNS for stocks (instead of raw prices)
returns = np.log(data[stocks] / data[stocks].shift(1)).dropna()

# 2. STANDARDIZE macroeconomic variables (z-score)
scaler_macro = StandardScaler()
macro_scaled = pd.DataFrame(
    scaler_macro.fit_transform(data[fred_variables.keys()]),
    index=data.index,
    columns=fred_variables.keys()
)

# 3. FINAL DATASET
final_data = returns.join(macro_scaled).dropna()
print("Final dataset after preprocessing shape:", final_data.shape)

# --- CORRELATION HEATMAP ---
plt.figure(figsize=(14,10))
sns.heatmap(final_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Variables')
plt.show()

# ------------------------------------------
# BLOCK 2: Traditional Time Series Models
# ARIMA (Univariate) and VAR (Multivariate)
# ------------------------------------------

import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.api import VAR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

# --- FUNCTIONS to Evaluate Models ---
def evaluate_model(y_true, y_pred, model_name='Model'):
    mse = mean_squared_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f'\nEvaluation Metrics for {model_name}:')
    print(f'  MSE  : {mse:.6f}')
    print(f'  RMSE : {rmse:.6f}')
    print(f'  MAE  : {mae:.6f}')
    print(f'  MAPE : {mape:.6f}')
    print(f'  R²   : {r2:.6f}')
    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}

# -----------------------------------
# A. ARIMA (on AAPL example)
# -----------------------------------
print("\nTraining ARIMA model on AAPL...")

# We will take only 'AAPL' to start
ts = data_stocks['AAPL']

# Split into train/test
train_size = int(len(ts) * 0.8)
train, test = ts[:train_size], ts[train_size:]

# Fit ARIMA (you can optimize order=(p,d,q) but here a simple one)
arima_model = ARIMA(train, order=(5,1,0))
arima_fit = arima_model.fit()

# Forecast
forecast_arima = arima_fit.forecast(steps=len(test))

# Evaluate
arima_scores = evaluate_model(test, forecast_arima, model_name='ARIMA (AAPL)')

# Correct evaluation function
def evaluate_model(y_true, y_pred, model_name='Model'):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f'\nEvaluation Metrics for {model_name}:')
    print(f'  MSE  : {mse:.6f}')
    print(f'  RMSE : {rmse:.6f}')
    print(f'  MAE  : {mae:.6f}')
    print(f'  MAPE : {mape:.6f}')
    print(f'  R²   : {r2:.6f}')
    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2}


# ARIMA part
print("\nTraining ARIMA model on AAPL...")

ts = data_stocks['AAPL']

train_size = int(len(ts) * 0.8)
train, test = ts[:train_size], ts[train_size:]

arima_model = ARIMA(train, order=(5,1,0))
arima_fit = arima_model.fit()

forecast_arima = arima_fit.forecast(steps=len(test))

arima_scores = evaluate_model(test, forecast_arima, model_name='ARIMA (AAPL)')


# VAR part
print("\nTraining VAR model on all stocks...")

diff_data = data_stocks.diff().dropna()

train_size_var = int(len(diff_data) * 0.8)
train_var, test_var = diff_data[:train_size_var], diff_data[train_size_var:]

var_model = VAR(train_var)
lag_order = var_model.select_order().aic
var_fit = var_model.fit(lag_order)

forecast_var = var_fit.forecast(train_var.values[-lag_order:], steps=len(test_var))
forecast_var_df = pd.DataFrame(forecast_var, index=test_var.index, columns=test_var.columns)

var_scores = evaluate_model(test_var['AAPL'], forecast_var_df['AAPL'], model_name='VAR (AAPL)')

import pandas as pd
import numpy as np
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.api import VAR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assume `data_stocks` and `data_macro` are your original datasets
# Example: data_stocks = pd.read_csv('stocks.csv', index_col=0, parse_dates=True)

# 1. DIFFERENCING
data_stocks_diff = data_stocks.diff().dropna()
data_macro_diff = data_macro.diff().dropna()

print("\nData differenced successfully!")

# 2. ADF Test Function
def adf_test(series, title=''):
    result = adfuller(series, autolag='AIC')
    print(f'ADF Test for {title}:')
    print(f'  Test Statistic: {result[0]:.4f}')
    print(f'  p-value: {result[1]:.4f}')
    print(f'  {"Stationary" if result[1] <= 0.05 else "Non-Stationary"}\n')

# 3. RUN ADF TESTS
print("--- ADF Tests for Differenced Stocks ---")
for col in data_stocks_diff.columns:
    adf_test(data_stocks_diff[col], title=col)

print("--- ADF Tests for Differenced Macroeconomic Variables ---")
for col in data_macro_diff.columns:
    adf_test(data_macro_diff[col], title=col)

# 4. TRAIN NEW ARIMA MODEL (example on AAPL)
train_size = int(len(data_stocks_diff) * 0.8)
train, test = data_stocks_diff['AAPL'].iloc[:train_size], data_stocks_diff['AAPL'].iloc[train_size:]

model_arima = ARIMA(train, order=(1,0,1))  # (p,d,q) with d=0 because already differenced
model_arima_fit = model_arima.fit()

forecast_arima = model_arima_fit.forecast(steps=len(test))

# 5. TRAIN NEW VAR MODEL (on all stocks)
model_var = VAR(data_stocks_diff.iloc[:train_size])
model_var_fit = model_var.fit(maxlags=15, ic='aic')

forecast_var = model_var_fit.forecast(y=data_stocks_diff.iloc[:train_size].values, steps=len(test))
forecast_var_df = pd.DataFrame(forecast_var, index=test.index, columns=data_stocks_diff.columns)

# 6. EVALUATION METRICS FUNCTION
def evaluate(true, pred, model_name='Model'):
    mse = mean_squared_error(true, pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true, pred)
    r2 = r2_score(true, pred)
    print(f"\nEvaluation Metrics for {model_name}:")
    print(f"  MSE  : {mse:.4f}")
    print(f"  RMSE : {rmse:.4f}")
    print(f"  MAE  : {mae:.4f}")
    print(f"  R²   : {r2:.4f}")

# 7. EVALUATE
evaluate(test, forecast_arima, model_name='ARIMA (AAPL)')
evaluate(test, forecast_var_df['AAPL'], model_name='VAR (AAPL)')

import pandas as pd
import numpy as np
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assume data_stocks_diff already exists (differenced stock data)

# 1. TRAIN-TEST SPLIT
train_size = int(len(data_stocks_diff) * 0.8)
train, test = data_stocks_diff['AAPL'].iloc[:train_size], data_stocks_diff['AAPL'].iloc[train_size:]

# 2. AUTO-ARIMA
print("Running auto_arima optimization...")
model_auto_arima = auto_arima(
    train,
    start_p=0, start_q=0,
    max_p=5, max_q=5,
    seasonal=False,  # No seasonality for stock prices usually
    stepwise=True,
    suppress_warnings=True,
    information_criterion='aic',
    error_action='ignore'
)

print("\nBest ARIMA Order Found:", model_auto_arima.order)

# 3. FIT THE OPTIMAL MODEL
model_auto_arima.fit(train)

# 4. FORECAST
forecast_auto = model_auto_arima.predict(n_periods=len(test))

# 5. EVALUATE
def evaluate(true, pred, model_name='Model'):
    mse = mean_squared_error(true, pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true, pred)
    r2 = r2_score(true, pred)
    print(f"\nEvaluation Metrics for {model_name}:")
    print(f"  MSE  : {mse:.4f}")
    print(f"  RMSE : {rmse:.4f}")
    print(f"  MAE  : {mae:.4f}")
    print(f"  R²   : {r2:.4f}")

evaluate(test, forecast_auto, model_name='Auto-ARIMA (AAPL)')

!pip install pmdarima

import pandas as pd
import numpy as np
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assume data_stocks_diff already exists (differenced stock data)

# 1. TRAIN-TEST SPLIT
train_size = int(len(data_stocks_diff) * 0.8)
train, test = data_stocks_diff['AAPL'].iloc[:train_size], data_stocks_diff['AAPL'].iloc[train_size:]

# 2. AUTO-ARIMA
print("Running auto_arima optimization...")
model_auto_arima = auto_arima(
    train,
    start_p=0, start_q=0,
    max_p=5, max_q=5,
    seasonal=False,  # No seasonality for stock prices usually
    stepwise=True,
    suppress_warnings=True,
    information_criterion='aic',
    error_action='ignore'
)

print("\nBest ARIMA Order Found:", model_auto_arima.order)

# 3. FIT THE OPTIMAL MODEL
model_auto_arima.fit(train)

# 4. FORECAST
forecast_auto = model_auto_arima.predict(n_periods=len(test))

# 5. EVALUATE
def evaluate(true, pred, model_name='Model'):
    mse = mean_squared_error(true, pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(true, pred)
    r2 = r2_score(true, pred)
    print(f"\nEvaluation Metrics for {model_name}:")
    print(f"  MSE  : {mse:.4f}")
    print(f"  RMSE : {rmse:.4f}")
    print(f"  MAE  : {mae:.4f}")
    print(f"  R²   : {r2:.4f}")

evaluate(test, forecast_auto, model_name='Auto-ARIMA (AAPL)')

!pip install --upgrade pmdarima

!pip uninstall numpy -y
!pip uninstall pmdarima -y

!pip install pmdarima

import pandas as pd
import numpy as np
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Assuming you have the data for AAPL, V, WMT, etc.
# Let's say `AAPL` is your time series data.

# Example: Loading a time series (you can replace this with your actual data)
# Replace this with your actual data loading step
aapl_data = pd.Series([...])  # Replace with your AAPL data

# Fit AutoARIMA model
model = auto_arima(aapl_data, seasonal=False, stepwise=True, trace=True)

# Forecasting (let's assume we forecast the next 10 periods)
forecast = model.predict(n_periods=10)

# Evaluate the model performance
# Assuming you have a test set to compare (test_data)
# test_data = pd.Series([...])  # Replace with your actual test data

# Example: If you want to compare the forecast against some test data:
# mse = mean_squared_error(test_data, forecast)
# mae = mean_absolute_error(test_data, forecast)
# r2 = r2_score(test_data, forecast)

# Print results
print(f"Forecast: {forecast}")
# print(f"MSE: {mse}")
# print(f"MAE: {mae}")
# print(f"R²: {r2}")

pip install numpy==1.21.0

pip uninstall pmdarima
pip install pmdarima

# Block 3: Deep Learning Models (ANN, CNN, RNN, LSTM)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, SimpleRNN
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# 1. Data Preprocessing (scaling and train-test split)

# Assuming `data` is a DataFrame and 'target' is the column we're predicting
# Scaling the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Train-Test Split
train_size = int(len(scaled_data) * 0.8)
train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]

X_train, y_train = train_data[:, :-1], train_data[:, -1]
X_test, y_test = test_data[:, :-1], test_data[:, -1]

# Reshaping for LSTM (3D input: [samples, time steps, features])
X_train_LSTM = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test_LSTM = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# 2. ANN Model (Artificial Neural Network)
def build_ann():
    model = Sequential()
    model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='linear'))  # For regression
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# 3. RNN Model (Recurrent Neural Network)
def build_rnn():
    model = Sequential()
    model.add(SimpleRNN(64, input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2]), activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='linear'))  # For regression
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# 4. CNN Model (Convolutional Neural Network)
def build_cnn():
    model = Sequential()
    model.add(Conv1D(64, 3, activation='relu', input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2])))
    model.add(MaxPooling1D(2))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='linear'))  # For regression
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# 5. LSTM Model (Long Short-Term Memory)
def build_lstm():
    model = Sequential()
    model.add(LSTM(64, input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2]), activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='linear'))  # For regression
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# 6. Training and Evaluation (for each model)

# Helper function to train and evaluate models
def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=1)

    # Evaluate the model on test data
    test_loss = model.evaluate(X_test, y_test, verbose=0)
    print(f'Test Loss: {test_loss}')

    # Predictions
    predictions = model.predict(X_test)

    # Plotting predictions vs actual
    plt.figure(figsize=(10, 6))
    plt.plot(y_test, label='Actual')
    plt.plot(predictions, label='Predicted')
    plt.title('Predictions vs Actual')
    plt.legend()
    plt.show()

    return history, predictions

# Build and train each model

# 1. Train ANN
ann_model = build_ann()
print("Training ANN...")
history_ann, predictions_ann = train_and_evaluate(ann_model, X_train, y_train, X_test, y_test)

# 2. Train RNN
rnn_model = build_rnn()
print("Training RNN...")
history_rnn, predictions_rnn = train_and_evaluate(rnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

# 3. Train CNN
cnn_model = build_cnn()
print("Training CNN...")
history_cnn, predictions_cnn = train_and_evaluate(cnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

# 4. Train LSTM
lstm_model = build_lstm()
print("Training LSTM...")
history_lstm, predictions_lstm = train_and_evaluate(lstm_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

def build_cnn():
    model = Sequential()
    # Adjust the kernel size to (3, 1)
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(1, 13)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

def build_cnn():
    model = Sequential()
    # Adjust the kernel size to (3, 1) to match input shape
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(1, 13)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Example code to train the CNN model
cnn_model = build_cnn()
print("Training CNN...")
history_cnn, predictions_cnn = train_and_evaluate(cnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

def build_cnn(input_shape):
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Check the shape of your data
print(X_train_LSTM.shape)  # Should be (samples, timesteps, features)

# Adjust the CNN input shape accordingly
cnn_model = build_cnn(input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2]))
print("Training CNN...")
history_cnn, predictions_cnn = train_and_evaluate(cnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

def build_cnn(input_shape):
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=input_shape))  # Use kernel_size=1 for 1 time step
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(1, activation='linear'))
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Check the shape of your data
print(X_train_LSTM.shape)  # Should be (1760, 1, 13)

# Adjust the CNN input shape accordingly
cnn_model = build_cnn(input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2]))
print("Training CNN...")
history_cnn, predictions_cnn = train_and_evaluate(cnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

def build_cnn(input_shape):
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=input_shape))  # Use kernel_size=1 for 1 time step
    model.add(Flatten())  # Flatten the output of Conv1D to feed into Dense layers
    model.add(Dense(1, activation='linear'))  # Output layer
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

# Check the shape of your data
print(X_train_LSTM.shape)  # Should be (1760, 1, 13)

# Adjust the CNN input shape accordingly
cnn_model = build_cnn(input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2]))
print("Training CNN...")
history_cnn, predictions_cnn = train_and_evaluate(cnn_model, X_train_LSTM, y_train, X_test_LSTM, y_test)

